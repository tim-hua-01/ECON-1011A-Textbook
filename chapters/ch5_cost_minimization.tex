\chapter{Cost Minimization} \label{ch:cost_minimization}
So far, we have dealt with firms choosing the inputs that will maximize the profit that is earned. There have been no restrictions on how much of the good needs to be produced other than that some quantities will yield higher profits than others. However, often times firms cannot produce as much as they want, and must produce a certain quantity. For example, a farmer may sign a contract to produce $1,000$ bushels of wheat by the end of the year for some fixed price. In these cases, the firm is not solving an unconstrained maximization problem, but instead they face a constraint of producing a fixed amount of good. The way for the firm to maximize profits if they must produce a fixed quantity of product is to minimize the cost of producing that quantity, which is known as a \vocab{cost minimization} problem

In this chapter, we will go over how to perform cost minimization, as well as why cost minimization can be useful in solving general profit maximization problems. 

\section{Problem setup}
To set up the cost minimization problem, we need to first establish our production function. For simplicity, we will assume that the firm has production function $f(K, L)$ where $K$ is capital and $L$ is labor. We assume that $f$ is increasing and concave with respect to both $K$ and $L$. That is,
\begin{align*}
    f_K &> 0 \\
    f_L &> 0 \\ 
    \frac{\partial^2 f}{\partial K^2} &< 0 \\
    \frac{\partial^2 f}{\partial L^2} &< 0
\end{align*}

The cost of labor is $r$, and the cost of labor is $w$, with both exogenous. We also have an exogenous quantity, $Q$, of goods that must be produced. The total cost of inputs is given by $wL + rK$. So we can write our minimization problem as,
\begin{align*}
    \min_{K, L} rK  + wL \text{ s.t. } f(K, L) = Q
\end{align*}
This says that we are choose $K$ and $L$ to minimize $rK + wL$ subject to the constraint that the amount we produce, $f(K, L)$, is equal to $Q$. To do so, we use constrained optimization. The Lagrangian is given by
\begin{align*}
    \Lagr(K, L, \lambda) = rK + wL - \lambda(f(K, L) - Q)
\end{align*}
We can solve this via our standard constrained optimization methods.

\subsection*{First order conditions}
We take the first order conditions on the Lagrangian, differentiating with respect to each variable, to obtain necessary conditions for a minimum, 
\begin{align*}
    \partials{\Lagr}{K} &= r - \lambda f_K(K, L) = 0 \\
    \partials{\Lagr}{L} &= w - \lambda f_L(K, L) = 0 \\
    \partials{\Lagr}{\lambda} &= f(K, L) - Q = 0
\end{align*}
Let $L^*$, $K^*$, and $\lambda^*$ denote the values that satisfy the above conditions. Note that the third condition is simply the constraint, $f(K^*, L^*) = Q$. However, we can also rearrange and divde the first and second constraints to obtain,
\begin{align}
    \frac{r}{w} = \frac{f_K(K^*, L^*)}{f_L(K^*, L^*)} \implies \frac{f_K(K^*, L^*)}{r} = \frac{f_L(K^*, L^*)}{w} \label{eq:marginal_cost_equal}
\end{align}
$f_K$ and $f_L$ tell us how much additional good is produced per unit of capital and labor respective, while $r$ and $w$ tell us how much an additional unit of each costs. The above equality tells us that, at an optimum, the additional good produced per dollar spent must be equal for capital and for labor. 

Now, consider $rK^* + wL^*$, where $K^*$ and $L^*$ are both functions of $r$, $w$, and $Q$. This tells us the total cost of producing $Q$ units of good. We can then define 
\begin{align*}
    C(Q; r, w) = rK^* + wL*
\end{align*}
This is known as the \vocab{cost function}, and it tells us the minimum cost to produce $Q$ units of good. In the next section, we will prove some important properties of the cost function. 

\TODO{Add some examples of cost minimization with specific functional forms}

\section{Cost function} 
Now that we have defined the cost function, we can examine some properties that it must exhibit. To do so, it will be useful to use the \vocab{constrained envelope theorem}.

\begin{theorem*}[Constrained Envelope] \label{thm:constrained_envelope}
    Let $F(x, y; z)$ be an objective function with choice variables $x, y$ and exogenous variable $z$, and let $g(x, y; z) = c$ be the constraint. Denote the optimal choices of $x$ and $y$ by $x^*(z)$ and $y^*(z)$, respectively. Let $v(z) = F(x^*(z), y^*(z); z)$. Then,
    \begin{align*}
        \frac{dv}{dz}(z) = \partials{\Lagr}{z}(x^*, y^*, \lambda^*; z) = F_z(x^*, y^*; z) - \lambda^* g_z(x^*, y^*; z)
    \end{align*}
    Where $\lambda^*$ is the value of the Lagrange multiplier that satisfies the first order conditions.  \footnote{This formulation of the theorem is dependent on how you write the Lagrangian. We write the Lagrangian in this text as, $\Lagr(x, y; z) = F(x, y; z) - \lambda(g(x, y; z) - c)$. However, it is also sometimes written as $\Lagr(x, y; z) = F(x, y; z) + \lambda(g(x, y; z) - c)$ (with addition instead of subtraction). These are equivalent except for the fact that the sign of $\lambda^*$ will flipped between them. So, for the latter formulation, we would have, $\frac{dv}{dz}(z) = F_z(x^*, y^*; z) - \lambda^* g_z(x^*, y^*; z)$}
\end{theorem*}

\begin{proof}
    First, it will be useful to recall the first order conditions for the Lagrangian,
    \begin{align*}
        \partials{\Lagr}{x} = 0 &\implies F_x(x^*, y^*; z) = \lambda^* g_x(x^*, y^*; z) \\
        \partials{\Lagr}{y} = 0 &\implies F_y(x^*, y^*; z) = \lambda^* g_y(x^*, y^*; z) \\
        \partials{\Lagr}{\lambda} = 0 &\implies g(x^*, y^*; z) = c
    \end{align*}
    Next, the value function is given by
    \begin{align*}
        v(z) = F(x^*(z), y^*(z); z) 
    \end{align*}
    Totally differentiating $v$ with respect to $z$ yields,
    \begin{align*}
        \frac{dv}{dz}(z) = F_x(x^*, y^*; z) \frac{dx^*}{dz} + F_y(x^*, y^*; z) \frac{dy^*}{dz}(z) + F_z(x^*, y^*; z)
    \end{align*}
    Now, notice that we can replace $F_x$ and $F_y$ using the first two equations in the FOC,
    \begin{align*}
        \frac{dv}{dz}(z) &= \lambda^* g_x(x^*, y^*; z)  \frac{dx^*}{dz} + \lambda^* g_y(x^*, y^*; z) \frac{dy^*}{dz} + F_z(x^*, y^*; z) \\
        &= \lambda^* \left(g_x(x^*, y^*; z) \frac{dx^*}{dz} + g_y(x^*, y^*; z) \frac{dy^*}{dz}\right) + F_z(x^*, y^*; z)
    \end{align*}
    Now, we totally differentiate the third equation in the FOC with respect to $z$ to obtain,
    \begin{align*}
        &\frac{d}{dz}g(x^*(z), y^*(z); z) = \frac{d}{dz} c \\
        \implies &g_x(x^*, y^*; z) \frac{dx^*}{dz} + g_y(x^*, y^*; z) \frac{dy^*}{dz} + g_z(x^*, y^*; z) = 0 \\
        \implies& g_x(x^*, y^*; z) \frac{dx^*}{dz} + g_y(x^*, y^*; z) \frac{dy^*}{dz} = - g_z(x^*, y^*; z)
    \end{align*}
    Plugging into the expression for $\frac{dv}{dz}$ yields,
    \begin{align*}
        \frac{dv}{dz}(z) = F_z(x^*, y^*; z) - \lambda^* g_z(x^*, y^*; z)
    \end{align*}
\end{proof}

With the constrained envelope theorem at hand, we can now examine some useful properties of the cost function.

\subsection*{Properties of the cost function} \label{sec:cost_properties}
First notice that the cost function $C(Q; r, w)$ is a value function, so the equivalent of $v$ in the statement of the constrained envelope theorem. The statement and the intuition of these properties will be the most important to remember, although the proofs may be helpful in better understanding methods of economic reasoning.
\begin{description}
    \item[Shephard's Lemma] $\frac{dC}{dr} = K^*(Q, r, w), \frac{dC}{dw} = L^*(Q, r, w)$. This is similar to Hotelling's Lemma, but tells us that as the price of an input increases, the cost increases by the amount that input is used.
    
    \begin{proof}
        Shephard's lemma is a straightforward application of the constrained envelope theorem,
        \begin{align*}
            \frac{dC}{dr} &= \frac{d\left(rK^* + wL^*\right)}{dr} - \lambda^* \partials{f}{r} = K^* \\
            \frac{dC}{dw} &= \frac{d\left(rK^* + wL^*\right)}{dw} - \lambda^* \partials{f}{w} = L^*
        \end{align*}
        Where $\partials{f}{r} = \partials{f}{w} = 0$ since the production function does not directly depend on $r$ or $w$.
    \end{proof} 
    \item[Homogeneous of degree 1 in input prices] $C(Q; \alpha r, \alpha w) = \alpha C(Q; r, w)$ for $\alpha \geq 0$. The intuition is that we are merely changing the unit of currency with which we are calculating costs. 
    
    \begin{proof}
        The first order conditions from $\ref{eq:marginal_cost_equal}$ requires that,
        \begin{align*}
            \frac{\alpha r}{\alpha w} = \frac{r}{w} = \frac{f_K(K^*, L^*)}{f_L(K^*, L^*)}
        \end{align*}
        The constraint does not depend on $r$ or $w$, so since the first order conditions are the same, we must have the optimized quantities are the same,
        \begin{align*}
            K^*(Q, \alpha r, \alpha w) = K^*(Q, r, w), L^*(Q, \alpha r, \alpha w) = L^*(Q, r, w)
        \end{align*}
        Plugging into the cost function yields,
        \begin{align*}
            C(Q; \alpha r, \alpha w) &= \alpha rK^*(Q, \alpha r, \alpha w) + \alpha w L^*(Q, \alpha r, \alpha w)\\
            &= \alpha r K^*(Q, r, w) + \alpha w L^*(Q, r, w) \\
            &= \alpha (rK^*(Q, r, w) + w L^*(Q, r, w)) \\
            &= \alpha C(Q; r, w)
        \end{align*}
    \end{proof}
    \item[Concave in input prices] Using the more mathematically formal definition of a concave function, this states that
    \begin{align*}
        C(Q, \alpha r_1 + (1 - \alpha) r_2, \alpha w_1 + (1 - \alpha)w_2) \geq &\alpha C(Q, r_1,  w_1) + (1 - \alpha)C(Q, r_2, w_2)
    \end{align*}
    Where $\alpha \in [0, 1]$. This tells us that the cost of the average of two prices is greater than the average of the costs at each price individually. The logic here is the exact same as the logic for the convexity of the profit function. Because firms can reoptimize, they have lower costs at any two prices of inputs than if they had any weighted average of the two prices. 

    \begin{proof}
    This is essentially equivalent to the profit function being convex in prices, and the proof is also basically the same. Let $\vec{w}_1 = (r_1, w_1)$ and $\vec{w}_2 = (r_2, w_2)$ be vectors of the input prices and let $\alpha \in [0, 1]$. Denote $\vec{w} = \alpha \vec{w}_1 + (1 - \alpha) \vec{w}_2$. Let $X^*(Q, r, w) = (K^*(Q, r, w), L^*(Q, r, w)$ be the vector of optimal choices. Notice that we can then write the cost function as a dot product, $C(Q, \vec{w}) = X^*(Q, \vec{w}) \cdot \vec{w}$. Then we have,
    \begin{align*}
        C(Q, \vec{w}) &= X^*(Q, \alpha \vec{w}_1+ (1 - \alpha) \vec{w}_2) \cdot (\alpha \vec{w}_1+ (1 - \alpha) \vec{w}_2) \\
        &= X^*(Q, \vec{w}) \cdot \alpha \vec{w}_1 + X^*(Q, \vec{w}) \cdot (1 - \alpha) \vec{w}_2
    \end{align*}
    Now, note that $X^*(Q, \alpha \vec{w}_1) \cdot \alpha \vec{w}_1$ is the cost function when we have input prices $\alpha \vec{w}_1$ and must be, by definition of the cost function, the minimum possible amount we spend to produce $Q$ at prices $\alpha \vec{w}_1$. This means that $X^*(Q, \vec{w})$ must not be the best choice of inputs at prices $\alpha \vec{w}_1$, so the cost must be higher. That is, 
    \begin{align*}
        X^*(Q, \vec{w}) \cdot \alpha \vec{w}_1 \geq X^*(Q, \alpha \vec{w}_1) \cdot \alpha \vec{w}_1 = C(Q, \alpha \vec{w}_1)
    \end{align*}
    The same must also hold for $(1 - \alpha) \vec{w}_2$. So,
    \begin{align*}
        C(Q, \vec{w}) &= X^*(Q, \vec{w}) \cdot \alpha \vec{w}_1 + X^*(Q, \vec{w}) \cdot (1 - \alpha) \vec{w}_2 \\
        &\geq X^*(Q, \alpha \vec{w}_1) \cdot \alpha \vec{w}_1 + X^*(Q, (1 - \alpha)\vec{w}_2) \cdot (1 - \alpha)\vec{w}_2 \\
        &= C(Q, \alpha \vec{w}_1) + C(Q, (1 - \alpha) \vec{w}_2) \\
        &= \alpha C(Q, \vec{w}_1) + (1 - \alpha)C(Q, \vec{w}_2) \text{ because homogeneous degree 1}
    \end{align*}
    \end{proof}
    \item[Inputs decrease with price increase] $\frac{dK^*}{dr} \leq 0, \frac{dL^*}{dw} \leq 0$. That is, as the price of an input increases, we must use weakly less of that input.
    
    \begin{proof}
        The easiest way to see this is using the fact that the cost function is concave. Notice that using the envelope theorem, we have that
        \begin{align*}
            \frac{dC}{dr} = K^*
        \end{align*}
        Then, differentiating again, we get the second derivative as,
        \begin{align*}
            \frac{d^2C}{dr^2} = \frac{dK^*}{dr}
        \end{align*}
        Because $C$ is concave with respect to $r$, we have that $\frac{d^2C}{dr^2} = \frac{dK^*}{dr} < 0$. And the same logic applies for $\frac{dL^*}{dw}$. 
    \end{proof}

    \item[Costs increasing in quantity] $\frac{dC}{dQ} > 0$, and in particular, $\frac{dC}{dQ} = \lambda^*$ where $\lambda^*$ is the value of the Lagrange multiplier that satisfies the first order conditions. This is also known as the \vocab{shadow cost} of the constraint, which tells us how much costs increase for a small increase in the constraint $Q$. 
    \begin{proof}
        First, we show that $\frac{dC}{dQ} = \lambda^*$. This follows from the constrained envelope theorem,
        \begin{align*}
            \frac{dC}{dQ} &= \partials{\Lagr}{Q}(K^*, L^*, \lambda^*)\\
            &= \partials{(rK + wL - \lambda(f(K, L) - Q))}{Q}(K^*, L^*, \lambda^*) \\
            &= \lambda^*
        \end{align*}
        To show that $\lambda^* > 0$, we can look at the first order conditions of the Lagrangian:
        \begin{align*}
            r = \lambda^* f_K \implies \lambda^* = \frac{r}{f_K}
        \end{align*}
        By assumption, $r > 0$ and $f_K > 0$, so we know that $\lambda^* > 0$. 
    \end{proof} 

    \item[Costs convex in quantity] $\frac{d^2C}{dQ^2} > 0$. That is, as the amount of goods we must produce increases, so does the marginal cost, assuming that the production function is concave. Intuitively, a concave production function means that as we need produce more, we need more of the inputs to produce each additional unit of the output. This is the same as the cost of each additional unit increasing.
    
    As a warning, the proof for this statement is longer than some of the other proofs and a little more confusing, so it is not important that you fully understand it. It is far more important to understand the intuition behind why convex cost functions and concave production functions are really the same thing. However, the proof may be useful to better understanding this intuition is formalized and common approaches to proving statements in mathematical economics. 
    \begin{proof}
        To prove this fact, we will again use the more mathematically formal definition of convexity. Let $Q_1$ and $Q_2$ be two quantities of the good, and let $0 \leq \alpha \leq 1$. To prove convexity, we must show that 
        \begin{align*}
            C(\alpha Q_1 + (1 - \alpha) Q_2) \leq \alpha C(Q_1) + (1 - \alpha) C(Q_2)
        \end{align*}
        We will use vector notation to make the proof fully general and save some space on notation. Let $\vec{w}$ be the vector of the prices of the inputs, and let $\vec{X}^*$ be the vector of inputs that achieves the minimum cost $C(\alpha Q_1 + (1 - \alpha) Q_2)$. Let $\vec{X}_1^*$ and $\vec{X}_2^*$ be the cost minimizing inputs to produce $Q_1$ and $Q_2$, respectively. 

        We can then write our cost function as,
        \begin{align*}
            C(\alpha Q_1 + (1 - \alpha) Q_2) &= \vec{w} \cdot \vec{X}^*
        \end{align*}
        First, suppose we knew that if we used $\alpha \vec{X}_1^* + (1 - \alpha) \vec{X}_2^*$ inputs, then we could produce at least $\alpha Q_1 + (1 - \alpha) Q_2$ output. This is not obvious, and we will show it soon. However if we did know this, then we know that using $\alpha \vec{X}_1^* + (1 - \alpha) \vec{X}_2^*$ must have a higher cost than using $\vec{X}^*$, because $\vec{X}^*$ is the set of inputs with minimum cost to produce at least $\alpha Q_1 + (1 - \alpha) Q_2$ of the good. Mathematically, this says,
        \begin{align*}
            C(\alpha Q_1 + (1 - \alpha) Q_2) &= \vec{w} \cdot \vec{X}^* \\
            &\leq \vec{w} \cdot (\alpha \vec{X}_1^* + (1 - \alpha) \vec{X}_2^*) \\
            &= \alpha (\vec{w} \cdot \vec{X}_1^*) + (1 - \alpha) (\vec{w} \cdot \vec{X}_2^*)
        \end{align*}
        However, notice that $\vec{w} \cdot \vec{X}_1^*$ is simply the minimum cost to produce $Q_1$, and same for $\vec{X}_2^*$ and $Q_2$. So, we can rewrite the inequality, 
        \begin{align*}
            C(\alpha Q_1 + (1 - \alpha) Q_2) &\leq \alpha (\vec{w} \cdot \vec{X}_1^*) + (1 - \alpha) (\vec{w} \cdot \vec{X}_2^*) \\
            &= \alpha C(Q_1) + (1 - \alpha)C(Q_2)
        \end{align*}
        Which is precisely the definition of a convex function. Now, we simply need to show that by using inputs $\alpha \vec{X}_1^* + (1 - \alpha) \vec{X}_2^*$, we could produce at least $\alpha Q_1 + (1 - \alpha) Q_2$ output. To do so, we simply use the fact that the profit function is concave (reverse the inequality in the convexity definition), to obtain,
        \begin{align*}
            f(\alpha \vec{X}_1^* + (1 - \alpha) \vec{X}_2^*) \geq \alpha f(\vec{X}_1^*) + (1 - \alpha) f(\vec{X}_2^*)
        \end{align*}
        However, recall how we defined $\vec{X}_1^*$ and $\vec{X}_2^*$. They are the inputs that would produce $Q_1$ and $Q_2$, respectively. So, plugging in that fact to the above inequality yields 
        \begin{align*}
            f(\alpha \vec{X}_1^* + (1 - \alpha) \vec{X}_2^*) &\geq \alpha f(\vec{X}_1^*) + (1 - \alpha) f(\vec{X}_2^*) \\
            &= \alpha Q_1 + (1 - \alpha) Q_2
        \end{align*}
        This is exactly what we wanted to prove, so we are finished.
    \end{proof}
    
\end{description}

\section{Duality of Profit Maximization}
So far, we have dealt with the cost function in a circumstance where the firm may be required to produce some amount $Q$ of the output for outside reasons. However, the cost function is also useful in a general profit maximization problem. In fact, the problem of cost minimization is the \emph{exact same} as the problem of unconstrained profit maximization. This is known as \vocab{duality}, which is when a maximization problem can be converted into an equivalent minimization problem, and vice versa.

\subsection*{How are they dual problems?}
To understand duality, let's first take a closer look at the problem of profit maximizatoin. Typically when we set up a profit maximization problem, we choose how much of each input to use, say how many workers to hire or how much capital to buy. However, through this choice of inputs, we are also implicitly choosing how much output to produce. This means that there is some profit maximizing quantity. Let's call this quantity $Q^*$.

Now the relationship to cost minimization becomes clearer. Suppose we knew that to maximize profits, we would have to produce $Q^*$ of the good. Then the inputs that we choose must achieve the minimum cost to produce $Q^*$, which is $C(Q^*)$. Why is this the case? Well suppose we chose some other set of inputs to produce $Q^*$. Since we sell $Q^*$ for the same price no matter what, we could strictly increase profits by switching to the cost minimizing set of inputs to produce $Q^*$. 

In fact, the same logic tells us that no matter how much we produce, we would make the most profit by producing with the cost minimizing set of inputs. So, we could instead see the profit maximization problem not as choosing the inputs, but choosing the quantity. We can treat the cost function as a machine that essentially tells us how much it costs to produce some quantity, and we would maximize with the cost minimization as given. That is, we could write the profit maximization problem as,
\begin{align*}
    \max_{Q} pQ - C(Q)
\end{align*}

While we will not provide a formal proof of the duality of the problems, the above intuition and reasoning should give you a good idea for why cost minimization and choosing the optimal quantity is the same as profit maximization by choosing precisely which inputs. 

\subsection*{Why use cost minimization?}
You may ask, if cost minimization and profit maximization solve the same problem, why do we need to cost minimize at all? After all, profit maximization seems simpler. Indeed, for most problems, it is a good bet that profit maximization will be easier to perform. After all, in profit maximization all you have is an unconstrained maximization problem rather than a constrained minimization problem followed by an unconstrained maximization problem.

However, there are still good reasons to care about the fact that these are dual problems. The first is that there are in fact some cases where cost minimization will be easier than profit maximization. This is because cost minimization only depends on the production function itself, not on how the good is sold. While in the perfectly competitive case that we have been dealing with so far this is not an issue, it can become more complicated once we reach a situation where the quantity produced also affects the price. In those cases, it may be easier to choose a quantity for a given cost, rather than having to choose the specific inputs when maximizing. In essence, cost minimization allows us to separate the problems of choosing inputs and choosing quantities.

The second reason that duality is useful is not necessarily as a problem solving mechanism in itself, but for choosing good models. For example, if you are writing a simple model of production, you may not care precisely how a good is produced, only that it has some associated cost. Many economic models will simply assume a cost function. The properties in the previous section tell us what a cost function would have to satisfy in order to represent a concave production function. We can therefore abstract away from the specific production process, and assume merely that each good produced has an associated cost defined by the cost function.

\section{More complex constraints}
So far, we have dealt with the case of a single equality constraint. However, we could have more complicated constraints, which we will discuss here.

\subsection*{Single inequality constraint}
So far, we have said that we need to produce \emph{exactly} $Q$ quantity of a good. However, the problem might be reformulated as needing produce \emph{at least} $Q$ of a good, in which case our problem would be
\begin{align*}
    \min_{K, L} rK + wL \text{ s.t. } f(K, L) \geq Q
\end{align*}
However, this does not in fact change our optimization problem at all. In fact, this is the exact same optimization problem as
\begin{align*}
    \min_{K, L} rK + wL \text { s.t. } f(K, L) = Q
\end{align*}
Why is this the case? Well suppose that if at the optimum, we produced not exactly $Q$, but some quantity $Q' > Q$. Could this ever be cost minimizing? The answer is no, because the cost function for producing exact quantities is an increasing function in $Q$. That is, we know that $C(Q') > C(Q)$. So, we could lower costs by producing slightly less than $Q'$, but still at a level above $Q$. This means that whatever arrangement we have of producing $Q'$ cannot be cost minimizing. Thus, when cost minimizing subject to an inequality constraint, the optimal choice must produce exactly the amount of quantity required.

\subsection*{Multiple equality constraints}
We dealt with the case that we had to produce exactly $Q$ of a single good. However, what if we had to produce multiple goods and produce an exact quantity of each? That is, suppose we instead had production functions $f_1, f_2, \dots, f_n$ for each good, and had to produce $Q_1, Q_2, \dots, Q_n$ of each good. An example where this may be the case is a firm needing to produce a fixed amount of good in each period, for $n$ periods. Let $X_1, \dots, X_k$ be the set of inputs, and let $w_1, \dots, w_k$ be the price of each input. 

To solve this problem, we once again use the Lagrangian, which in the multiple constraints case is given by
\begin{align*}
    \Lagr = \sum_{i = 1}^k X_i w_i - \sum_{j = 1}^n \lambda_j(f_j(\vec{X}) - Q_j)
\end{align*}
All that we have done is add additional Lagrange multipliers, $\lambda_j$, and then you solve by taking first order conditions the same way you would normally, except for each $\lambda_j$.

Notice one concern with the multiple constraint case that is not present in the single constraint case: we are not guaranteed that a solution exists. That is, it is not clear that in general there will be a choice of inputs that produces \emph{exactly} the required amount for each good. We may therefore require some restrictions, such as requiring that the inputs used on good 1 do not affect the production of good 2, and vice versa.

For the most part in this class, we will only deal with single equality constraints, but it may be useful to know how to handle more equality constraints.

\subsection*{Multiple inequality constraints}
A final complication is the case of multiple inequality constraints. This is considerably more difficult than either the multiple equality constraints or the single inequality constraint, because we may not satisfy the constraints with equality and we do not know how much extra we would produce. The way to solve a problem with multiple constraints is given by the \vocab{Karush-Kuhn-Tucker} (KKT) conditions, which are a set of necessary conditions for an optimum. 

The KKT conditions are far beyond the scope of this course, and so we will not even describe them here. However, it is useful to know the problem that they solve so that you can know where to look in case you may find them helpful when developing your own model or when you read about the KKT conditions in an academic publication. 

\section*{Recap}
In this chapter, we detailed how to setup and solve a cost minimization problem to obtain the cost function $C(Q)$, for producing exactly $Q$ output. We also described and proved some of the key properties that the cost function must satisfy. Finally, we showed how cost minimization is the dual problem of profit maximization. The techniques of constrained optimization that have been introduced in this section will be useful beyond cost minimization, and especially in the upcoming chapters on individual utility maximization. If you do not fully understand constrained optimization, it would be worth re-reading parts of this chapter, the Math Review \TODO{link to this}, or to obtain additional practice from other resources. 